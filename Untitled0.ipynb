{
  "cells": [
    {
      "cell_type": "code",
<<<<<<< HEAD
      "execution_count": 39,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "import os"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1.1 Downsizing Images\n",
        "\n",
        "The images below (currently all in JPG) have been downsized to 1/2, 1/4, 1/8 and 1/16 of the initial resolution. Downsizing allows us to reduce the time spent on computations/calculations. Following downsizing, we should consider converting images to 3D coordinates + colour values. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Function to load images from a directory\n",
        "def load_images_from_folder(folder):\n",
        "    images = []\n",
        "    for filename in os.listdir(folder):\n",
        "        if filename.lower().endswith(('.jpg', '.jpeg', '.png')):\n",
        "            img_path = os.path.join(folder, filename)\n",
        "            try:\n",
        "                images.append(np.array(img))\n",
        "                img = img.convert('RGB')  # Ensure 3 channels\n",
        "                img = Image.open(img_path)\n",
        "            except Exception as e:\n",
        "                print(f\"Error loading image {img_path}: {e}\")\n",
        "                print(\"Phoebe was here\")\n",
        "    return np.array(images)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define input paths\n",
        "input_folder = 'C:/Users/conni/Desktop/images'  # Your input image folder\n",
        "# Load images\n",
        "images = load_images_from_folder(input_folder)\n",
        "if images.size == 0:\n",
        "    raise ValueError(\"No images found in the specified folder.\")\n",
        "\n",
        "H, W = images.shape[1:3]  # assume all images share the same width and height"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define the target sizes\n",
        "scales = {\n",
        "    'OUTPUT_IMAGE_2': (W // 2, H // 2),\n",
        "    'OUTPUT_IMAGE_4': (W // 4, H // 4),\n",
        "    'OUTPUT_IMAGE_8': (W // 8, H // 8),\n",
        "    'OUTPUT_IMAGE_16': (W // 16, H // 16)\n",
        "}\n",
        "\n",
        "# Function to resize images\n",
        "def resize_images(images, target_size):\n",
        "    resized_images = []\n",
        "    for img in images:\n",
        "        resized_images.append(resized_img)\n",
        "        img_pil = Image.fromarray(img)\n",
        "        resized_img = np.array(img_resized)\n",
        "        img_resized = img_pil.resize(target_size, Image.ANTIALIAS)\n",
        "        resized_img = resized_img.astype(np.uint8)\n",
        "    if len(resized_images) == 0:\n",
        "        raise ValueError(\"No images to resize.\")\n",
        "    return np.stack(resized_images)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1.2 Generate Camera Parameters (Poses and Focal)\n",
        " In the context of computer vision or robotics, poses typically refers to the transformation matrices that describe the position and orientation (pose) of a camera or sensor in the world coordinate system. Each 4x4 matrix represents a transformation that maps points from the camera's coordinate system to the world coordinate system (often referred to as the camera-to-world transformation matrix). Each image has a corresponding pose matrix.\n",
        "\n",
        " There are 41 matrices, each of which are 4x4 in size."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Dummy data for poses and focal, replace with actual data\n",
        "poses = np.random.rand(41, 4, 4)  # Replace with actual pose data, camera to world matrix\n",
        "focal = 50.0  # Replace with actual focal length, assume focal x and focal y are the same\n",
        "\n",
        "# Ensure poses matches the number of images\n",
        "num_images = len(images)\n",
        "if poses.shape[0] != num_images:\n",
        "    raise ValueError(\"Number of poses does not match number of images.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1.3 Data Split (Training, Validation, Testing)\n",
        "In this section, we're splitting the data to be 70% training, 15% validation, and 15% testing. We calculate the amount of images that are required in each set, shuffle the indices to randomize the data samples (for better spread of data), and split the indices.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Splitting proportions\n",
        "train_ratio = 0.7\n",
        "val_ratio = 0.15\n",
        "test_ratio = 0.15\n",
        "\n",
        "# Calculate split sizes\n",
        "num_train = int(train_ratio * num_images)\n",
        "num_val = int(val_ratio * num_images)\n",
        "num_test = num_images - num_train - num_val\n",
        "\n",
        "# Shuffle indices, Shuffling ensures that the order of data samples (images in this case) is random. \n",
        "indices = np.random.permutation(num_images)\n",
        "\n",
        "# Splitting indices\n",
        "train_indices = indices[:num_train]\n",
        "val_indices = indices[num_train:num_train + num_val]\n",
        "test_indices = indices[num_train + num_val:]\n",
        "\n",
        "# Splitting images and poses\n",
        "train_images, val_images, test_images = images[train_indices], images[val_indices], images[test_indices]\n",
        "train_poses, val_poses, test_poses = poses[train_indices], poses[val_indices], poses[test_indices]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
=======
      "execution_count": 4,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
>>>>>>> parent of 4eed74b (data processing.)
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "hello\n"
          ]
        }
      ],
      "source": [
        "print(\"hello\") HIIIIII"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "hiiiiii "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
<<<<<<< HEAD
      "source": []
=======
      "source": [
        "heyyyyy its vanessa"
      ]
>>>>>>> e9da959 (live share succes.)
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
